<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Dynamo: Facebook's Data Center-Wide Power Management System. ISCA2016]]></title>
      <url>%2F2017%2F04%2F09%2FDynamo-ISCA2016%2F</url>
      <content type="text"><![CDATA[&emsp;&emsp;这是ISCA2016的一篇论文《Dynamo: Facebook’s Data Center-Wide Power Management System》的分析。 论文章节大纲和观点摘要&emsp;&emsp;摘要部分主要说明了电力资源的稀缺性和重要性。论文提出了一个数据中心范围的监控和控制方案——Dynamo，Dynamo可以监控电力资源的层级并且给出协调的控制决策，Dynamo是用在了过去三年中Facebook所有的数据中心中。论文三个贡献分别为：提供了资源变化的特性；提出了Dynamo的细节设计；这个技术被部署和评价在服务于十亿级用户的大规模数据中心中。(注:power的翻译主要有：电力，功率或者电力功率，随上下文而变化) 引言(INTRODUCTION)&emsp;&emsp;在仓库级数据中心(Warehouse-scale data centers)中会有power delivery infrastructure为机器来提供电力，并且会使用power breakers来保护数据中心不被electrical surges损坏；但是tripping a power breaker会出现很严重的问题，使用过度供应(over-provisioning)的方案来解决这个问题可以提升安全性和可靠性，但是这种方法制造成本很高。使用Under-utilizing的数据中心则会导致数据中心效率降低。为了保证数据中心效率，有人提出了over-subscription的方法，但这会导致unpredictable power spikes。&emsp;&emsp;为了解决这种问题保证电力安全和电力的利用。有很多方法被提出，如：power-capping，peak power management techniques。与这些方法不同，本文作者关注于监控所有电力的层次并制定出协调的控制决策，并处理大规模数据中心问题的key issues，提出了Dynamo。&emsp;&emsp;Dynamo用来监控整个电力层级并且可以给出协调的控制决策使得可以安全有效地使用provisioned data center power。Dynamo的主要贡献如下： Dynamo提供了数据中心在不同workload下的变化特性。得出了为了抵抗现实中的电力故障问题，控制器电力读周期应该加快（近乎秒级完成）。 Dynamo描述了在真实环境下数据中心范围电力管理系统的实现。 Dynamo在服务于十亿级的大规模数据中心上经过了部署和评价。Dynamo已经被部署到了过去三年里所有的Facebook的数据中心中，它可以在很大程度了提升数据中心的电力利用。 背景(BACKGROUND)&emsp;&emsp;在背景一节中，作者从三个方面进行了展开，分别是：什么是data center delivery infrastructure，oversubscribe power意味着什么，在什么条件下oversubscription可以造成power outages。作者还讨论了设计数据中心电力管理系统时的影响因素(factor)意味着什么。&emsp;&emsp;其中分为A,B,C三节：&emsp;&emsp;A部分讲述了Fackbook数据中心的功率输出电力供应结构，其中Facebook的数据中心基于OCP规格，电力的供应为30MW。其中每个Main Switch Borads(MSBs)额定2.5MW的功率还会有备用的发电机在中断期间供应电力。一个数据中心会跨越4个房间(suits)，以行(row)为组织放在机架上。每个MSB的供应由4个1.25MW的Switch Boards(SBs)决定，每个SB给位于每行机架最后的(Peactive Power Panels)RPPs提供190KW。然后RPPs又会给Direct Current Uninterruptible Power Supplies(DCUPS)提供电力，DCUPS会给6个机架提供90秒的电力备份。还指出了低层级的电力供应设备相比于高层级设备会保持相对更多的电力透支(power withdraw)。A提出了两个重要观点： power capping的方法不足以来检测数据中心中的单个设备或者设备的子集 减小服务器性能的影响并且保持电力安全是很好的研究方向。 &emsp;&emsp;B部分关注于功率变化特性，为了对功率变化进行量化，作者定义了power slope，用来测量不同等级的功率层次下功率在一个特定时间窗口(3s到600s)的功率增长率。计算方法为：最差的功率变化情况下时间窗口内的最大最小功率值的差。作者观察了层次中每个等级(rack,RPP,SB,MSB)超过6个月的细粒度功率数据，通过把数据分片为2周数据然后联合结果的方法来简化数据。由于load multiplexing，在功率高层级的有更小的相对电力功率的变化。&emsp;&emsp;还有一个发现就是电力的变化也取决于应用。不同的应用有不同的电力变化特征。 &emsp;&emsp;C部分联合了A和B部分的结果，得到了一些实现数据中心功率管理系统的含义(implications)。提出了两个观点： 小于分钟级抽样(Sub-minute power sampling) ，即系统中的样本功率间隔应该处于小于分钟级 两分钟的功率限制时间，两分钟之内的间隔必须发出适当的上限命令并确保功率稳定。 Dynamo的体系架构overview&emsp;&emsp;在第三节主要讲述Dynamo的设计，分为综述和它的三个部分agent，leaf power controller和higher-level power controllers。在高层级来说的话Dynamo分为Dynamo agent和Dynamo controllers。其中agent是部署在数据中心每台服务器上的一个轻量级的程序，Dynamo agent只与Dynamo controllers通信，用于读取功率，执行power capping/uncapping命令。&emsp;&emsp;Dynamo controllers运行在一组专用服务器上，检测agents的数据是否被控制并对保护数据中心电力设备负责。controllers是分布式结构，反映数据中心电力层次的拓扑结构。Dynamo的结构中设备的最低层次是leaf power controller，它与agents直接通信，Thrift用来使得controllers和agents通信。leaf power controller用来减小power capping造成的总体性能影响。&emsp;&emsp;non-leaf level是upper-level power controller，它通过与leaf controllers或者other downstream upper-level controllers间接检测和控制下游服务器。 Agent Design&emsp;&emsp;Agent像一个守护进程，持续监听从leaf controller发来的请求，thrift作为其中的主要通信协议。agent掌控着两种基本类型的请求：Power read和Power capping/uncapping。&emsp;&emsp;Power read对于facebook的大部分机器都有传感器，一些小部分没有传感器的构建了功率估计模型，agent读取和返回当前主机服务器的消耗，可能的话会返回功率的故障情况。&emsp;&emsp;Power capping/uncapping，agent与Intel running average power limiting(RAPL)来通信实现装备或者复原电力限制。它也会返回操作状态给leaf controller，让leaf controller知道操作是否被成功执行了。 Leaf Power Controller Design&emsp;&emsp;它的功能包含三个部分：1)功率读取和聚合2)监控并制定capping决议3)选择最优服务器集合来cap使得最小化性能影响。 功率读取和聚合。leaf power controller会告诉agent功率的周期消耗的数据，它聚集功率的读来计算总的功率消耗。leaf power controller通过thrift广播功率，选择3秒作为pulling cycle，由于breaker的采样频率不快而且我们要知道单个服务器的功率消耗，所以不直接使用breaker。leaf power controller在聚合无效的时候会给人类的介入发出警告。 监控并制定capping决议。leaf power controller使用three-band算法来制定capping或者uncapping的决议，即：capping threshold，capping target，uncapping threshold，三段可以用来减小控制振幅，使得capping响应快来控制功率激增问题。 选择最优服务器集合来cap使得最小化性能影响。leaf power controller选择正确的服务器集来进行cap来减小性能影响，leaf power controller使用服务器控制的元数据，与功率消耗历史结合选择出合适的capping行为。 &emsp;&emsp;Fackbook的服务会分在预先定义的priority groups中，leaf power controller首先计算total-power-cut，然后分布属于低优先级的total-power-cut，然后再选择次低优先级依次执行。对于同等优先级的服务，使用high-bucket-first算法来决定每个服务器使用多少功率。即首先惩罚最耗费功率的，然后扩展bucket到下一个服务器，经过试验10W到30W的bucket size工作地比较好，当前使用的是20W的bucket size。 Upper-Level Power Controller Design/Coordination&emsp;&emsp;Upper-Level Power Controller也有着与Leaf Power Controller同样的funtion： 功率读取和聚合。upper-level power controller会定时pull功率读取。它从下游控制器中pull而不是直接从服务器中pull。其中pulling cycle是9秒。 Capping and uncapping decisions。capping decision logic使用同样的三段算法(three-band algorithm)来决定何时cap或者uncap。它的logic必须于eaf power controller的logic做协调来保证不冲突。 &emsp;&emsp;使用punish-offender-first算法来协调和选择upper-level power controller的正确行动，即如果有children device的功率之和违背了限制，那么该算法会削减其children device的功率，超过特定的children device称之为offender，然后该device会得到两个limit，一个是physical power limit(固有的limit)和contractual power limit(削减之后的limit)，然后使用两个limit较小的一个来完成capping decisions。 Dynamo Design Discussion&emsp;&emsp;Fault tolerance。容错方法有1)使用脚本来检查agent然后在agent crash的时候重启agent。2)如果power pulling failures超过了阈值，控制器会避免采取错误行为，替代发给人一个警告来介入的方法。当控制器crash的时候，使用存在于不同地点冗余备份来进行控制。&emsp;&emsp;Networking hardware。Dynamo会监控但不会控制网络设备的功率消耗，因为网络设备不支持RAPL-like power capping。&emsp;&emsp;Algorithm selection。使用简单三段算法来快速遍历设计过程并发现检测产品时候的问题，以后我们会探索更复杂的power capping算法。 结果：运作中的DynamoPower Capping&emsp;&emsp;以一个前端的位于Ashburn的集群为例，leaf controller保护几百个前端web服务器的PDU Breaker功率。发现上午8点到10点半总功率持续上升，上午10点40开始了更多traffic shifting。大概上午11:15超过了阈值触发了power capping，在6秒内leaf power controller使得功率到达了一个健康水平，到了下午12点总功率降到了leaf power controller的uncapping阈值，触发了uncapping。另外对于power surge，upper-level power controller进行保护并cap冒犯者，在8分钟内使得电力下降到限制，20分钟下降到一个安全区域。而且这些操作对于性能的影响微不足道 Dynamic Power Oversubscription&emsp;&emsp;Dynamo还可以提升Facebook的大规模hadoop集群的性能，Dynamo提供power safety net，可以安全地超额功率，使得可以打开Hadoop集群中所有服务器的Turbo Boost，这样服务器可以就利用overclocking提升性能。 Workload-Aware Power Capping&emsp;&emsp;leaf controller可以使用优先组选择出最优的capping action。Dynamo使用high-bucket-first算法来分别电力，bucket选择范围是[210W,300W]，total-power-cut会使用在功率消耗在210W或者以上。 Summary of Benefits&emsp;&emsp;本节总结了使用Dynamo得到的一些优势，主要为： 在过去6个月内组织了18次潜在的断电事故 提升了Hadoop集群的性能多达13% 提升了搜索的性能(优化搜索查询时间,QPS)到达40% 使得一个数据中心中相同的power delivery可以容纳8%更多的机器 细粒度的实时监控（功率读和故障之间三秒的间隔） 相关工作&emsp;&emsp;Dynamo的第一个在文献中描述的真实生产坏境中的数据中心范围功率管理系统。以往的研究关注于：峰值功率管理，热能管理，平均功率或者能源管理方法。以往大部分工作使用硬件方法：P-States或者DVFS。Dynamo关注于数据中心范围的峰值功率管理，它考虑整个功率层级并且于控制决策相协调。Dynamo试验在真实环境，并被部署到服务于十亿级用户的超大规模数据中心中。另外，Dynamo相比于其他研究，展现了混合的服务如何影响这些层级和时间尺度的变化。 学习到的经验教训&emsp;&emsp;监控和capping一样重要。监控对于发现和阻止功率问题很重要。&emsp;&emsp;业务感知系统设计简化了capping testing。service-aware可以帮助Dynamo来实施端对端的周期测试来保证Dynamo的正确性和可靠性。&emsp;&emsp;设计capping系统是一种硬件无关(hardware-agnostic)的方法。Dynamo提取软件为两层——平台独立和平台特性。这样使得主要逻辑对平台透明。也使得系统更具有扩展性。&emsp;&emsp;对于缺失的功率信息使用精确估计。Dynamo使用系统元数据精确地估计服务器功率的读故障。另一种方法是使用从power breaker得到粗粒度的功率读(reading)来验证和动态调整服务器功率估计和聚合。&emsp;&emsp;按尺度保持设计简单来达到可靠性。Dynamo的最优先的是保持可靠性，首先Dynamo保持agent和控制逻辑精简使得在发生问题的时候可以容易地辨别。再者，对于agent或者控制逻辑使用4期阶段转出，对于严重的问题可以在早期发现。在过去的三年里，Dynamo没有造成任何大的可靠性问题。 结论&emsp;&emsp;电力资源很紧缺很重要，Dynamo这种电力监控和管理系统就很重要。Dynamo可以帮助组织服务中断供应，使得动态功率oversubscription然后性能得到提升。Dynamo的监控框架给了功率使用和有效性的启示。Dynamo依然还有空间去提升。 论文内容分析简要总结解决什么问题&emsp;&emsp;Dynamo用来解决电力功率的变化造成的问题，如服务中断。这些问题对于业务有着很大影响，需要解决电力的使用异常情况，来保证数据中心的可靠性。包括一些很细节的东西，如capping的过程，整个监控系统的层次，各个模块之间如何进行协调。 key ideas/insights&emsp;&emsp;Dynamo的目的一个是为了减小断电损失，一个是为了提升性能。它的设计是一个architcture——主要包括Agent和Controller，Controller分为Leaf Power Controller和Upper-Level Power Controller，因此每个部分可以实现不同的功能。另外还要保证各部分协调，比如Dynamo采用了punish-offender-first算法来协调。Dynamo要保证有效的Capping和Monitoring，作者通过3年来Facebook的使用情况来保证的该系统的有效性。 Strengths&emsp;&emsp;该论文优点主要有： 给出了电力功率层级的变化特性 Dynamo的架构失效地解决了功率故障问题并提升了系统的性能 Dynamo的各个部分的协调工作使得可以有效监控Facebook数据中心的能源问题，并保证性能 Dynamo真正的部署在了实际的环境中，更有说服力，它保证了Facebook十亿级用户的业务安全和性能。 Weaknesses&emsp;&emsp;论文中没有涉及Dynamo的缺点，对于Dynamo的缺点，我个人的看法： 制定capping决议的三段算法比较简单，只分为了capping threshold，capping target，uncapping threshold，可以通过对历史数据的挖掘，得到更多的段，这样可以更好的来控制capping和uncapping 有些选择是依靠经验，比如high-bucket-first算法中的bucket的size，作者没有给出为什么要选择20W的size，是比较依靠经验的，对于这个选择可以进行比较：比如取15W,17.5W,20W,22.5W,25W进行比较。这样会得到更有说服力的结果。 作者只说了Dynamo没有造成大的问题，应该还是有其他问题存在的。原文：As a fact, Dynamo has not caused any big reliability issues in the past three years. 我对论文的观点(Can you do much better?)&emsp;&emsp;Dynamo是用来解决实际问题的，如果我来做，应该提出其他的方案来进行对比，比如thrift是不是可以换成其他的协议进行比较？软件方法(Dynamo)与硬件方法(DVFS)进行比较。另外关于上面提出的几个Weaknesses，在Capping的时候可不可以分更多的段来使得决策更精确。high-bucket-first算法的bucket size可不可以用理论或者统计的方法给出更好的结果。 我的所学&emsp;&emsp;我最大的所学就是一个真正的系统的重要性，就像老师说过USENIX很多会议都看能不能做出一个系统还不是看理论的推导。如果一个系统能保证十亿级用户服务的稳定性和可靠性，那么这个系统就是有效的。Dynamo就是这样的一个系统，它的每一步看起来并不是特别“高大上”，但是通过层次中每个部分的组合和协调，它可以变成一个真正有效的电力功率管理系统。&emsp;&emsp;在Dynamo中使用的算法没有很深的数学理论，但是它真正起到了作用（因为已经在过去三年里Facebook的数据中心一直在使用它达到了很好的效果）。对于系统领域，应该关注的是一个全局的目标，比如Dynamo关注监控功率，发现问题，提高性能。那么所有的部分要服务于这个目的，要追求精简和高效。就像文章最后提到的Keep the design simple to achieve reliability at scale。 Dynamo实验中的图分析Figure 5分析 实验目标&emsp;&emsp;实验目的是探究racks,RPPs,SBs和MSBs在不同时间窗口下的功率变化。从而知道功率的变化与时间窗口和电力功率层次之间的关系。 实验设计思想&emsp;&emsp;使用统计学的方法，绘制出CDF，然后比较曲线结果。分析不同层次在不同时间窗口的功率变化。 实验具体配置&emsp;&emsp;实验分析了不同的功率层次(rack,RPP,SB和MSB)的6个月的细粒度数据，然后把这些数据分片，分为两周的周期，联合多次周期的结果。 图中标记意义&emsp;&emsp;figure 5分为四个图，这四个图代表不同的功率层次(分别为Rack level,RPP level,SB level,MSB level),在不同层次里有6条曲线，分别代表不同的时间窗口，图中有对于的颜色标志，括号里面的百分比为功率变化。&emsp;&emsp;横坐标代表着功率变化的百分比，纵坐标是CDF(Cumulative Distribution Function,累计分布函数)。 实验结果&emsp;&emsp;从图中可以很直观的发现，时间窗口长度越大那么power变化百分比就越大。从rack到MSB的层次，层次不断上升，但是相对的power variation在降低。 结果解释 发现越大的时间窗口有着更大的功率变化 由于多路技术，越高的功率层次则有相对更小的功率变化 Figure 11分析 实验目标：&emsp;&emsp;通过观察位于Ashburn的前端集群的一排服务器的功率测量值来看leaf controller level 中的capping和uncapping的效果。 实验设计思想:&emsp;&emsp;观察前端集群的power变化，从上午8点到下午12点的情况，当到达阈值的时候会触发capping和uncapping，再观察power就可以知道capping和uncapping是否有效。 实验具体配置&emsp;&emsp;配置为服务于几百个web服务的前端服务器，PDU Breaker的额定功率为127.5KW。时间长度为上午8点到下午12点。起功能的层次为leaf controller level。 图中标记意义&emsp;&emsp;横坐标是时间，代表这一天几点钟，纵坐标则是对应的功率值。三条红色虚线就是三段算法中的——capping阈值，capping目标和uncapping阈值。另外有4个标记，分别是load increasing表示在这个时间点负载上升。然后capping triggered在这个时间点里触发了capping，load droping在这里表示高峰已过开始下降。uncapping trigger表示在这个时间点触发了uncapping。 实验结果&emsp;&emsp;在上午8点到10:30时候，总功率上升稳定，正式服务开始上升的阶段。到了上午10点40，更多的traffic shifting出现。最终到了额定的127KW，capping被触发，在6秒内功率就达到了安全的水平。然后上午11:45开始低于126KW，随后回到正常水平。大概到下午12点的时候，总的电压下降到了uncapping的阈值，触发了uncapping。 结果解释&emsp;&emsp;结果说明了在leaf controller level触发capping得到了很好的效果。如图所示，达到阈值经过capping之后，功率会明显地下跌。并回到正常水平，说明leaf controller level起到了很好的作用，同样地，在达到uncapping阈值后，使得功率得到了提升。 从上述论文中的收获&emsp;&emsp;这篇论文讲的是Facebook的数据中心电力功率管理系统，是一篇系统领域的佳作。从这篇论文中可以看出做系统研究的一些方法，如各个模块的分离和协调，分层次的结构，以及分析问题的方法；比如说统计学在系统领域有很大的用途，Dynamo中使用了CDF来观察不同因素对功率变化的影响。因为只有对数据进行统计，才知道可以设计出更有效的系统。&emsp;&emsp;Dynamo的很大特点就是创新和实用。创新指的是Dynamo可以吸取之前的研究中的不足和可以扩展的方向，它使用的是软件的方法不同于以往的硬件方法，Dynamo还给出了workload情况下的变化性质，这对于其他的研究者来说是个很重要的参考和创新。实用指的是Dynamo实际运用在了Facebook的大规模的数据中心，如果能在服务于十亿级用户的数据中心中运行的很好而且没有出现大的问题，就是Dynamo有很大价值的最好证明。在计算机系统结构研究中，实用性是最重要的，因为系统被研究出发就是要被使用的。&emsp;&emsp;此外，在写文章方面，Dynamo作为顶级会议的文章也有很多可学习之处。比如它的组织十分清晰，对于Dynamo描述的时候，先总体说一下再进行分层次的描述，在每个层次中又分为很多的小层次，这样就很清晰，知道每个层次在做什么，在每个小层次中又有比较清晰的说明。另外，在层次之间也有很联系，比如agent和controllers，其中controllers又分为Leaf Power Controller和Upper-Level Power Controller；再这两个controller里面又分为Power reading and aggregation和Capping and uncapping decisions。总之，读这篇文章受益匪浅，像Facebook，Google这样的大公司在系统领域研究中占有很大的比重，也说明了系统领域研究的实用性和重要性。]]></content>
    </entry>

    
  
  
</search>
